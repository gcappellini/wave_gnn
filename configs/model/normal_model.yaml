# Deep GCN Model configuration
in_channels: 3
hidden_channels: [64, 128, 256, 512]
out_channels: 2
conv_types: ["GEN", "GEN","GEN", "GEN"]
final_layer_type: "Linear"
activation: "relu"
dropout: 0.1
block: "plain" # recommended with heterogeneous dims; residual variants need equal in/out dims
use_bn: true
gat_heads: 4
cheb_K: 3
residual: false  # works only false

# Global pooling settings
# Set use_global_pooling=true to enable pooling. Choose where to apply it:
#  - "end": after all layers → graph-level output
#  - "middle": encoder-decoder with pooling bottleneck → node-level output with global context
use_global_pooling: true
pooling_position: middle   # middle | end
pooling_type: mean         # mean | max | sum | attention

# Encoder-decoder split (used when pooling_position: middle)
# If encoder_layers is null, it defaults to half of hidden_layers (here: 4)
encoder_layers: 4

# Optional custom decoder sizes. If null, it mirrors the encoder (reverse of first encoder_layers).
decoder_channels: null

# Optional bottleneck dimension after pooling. If null, uses encoder output dim (here: 512)
graph_output_dim: null
