# Training configuration
epochs: 10
learning_rate: 0.001
weight_decay: 1.0e-4
optimizer: "adam"  # "adam", "sgd", etc.

# Numerical stability
grad_clip_norm: 1.0  # Gradient clipping max norm (prevents exploding gradients)
scale_lr_with_dataset: true  # Automatically scale LR based on dataset size
lr_baseline_size: 1000  # Baseline dataset size for LR scaling (adjust to your typical num_graphs)

# Loss weights
loss:
  w1_PI: 1.0e+00 #1.323e+02  # weight for displacement loss
  w2_PI: 1.0e+00 #6.981e+02  # weight for velocity loss
  use_rk4: false
  w1_rk4: 1.0e+00
  w2_rk4: 1.0e+00
  use_gn_solver: false
  
  # Adaptive loss weighting
  adaptive:
    enabled: true  # Enable adaptive weighting
    strategy: 'fixed'  # Options: 'equal_init', 'equal_init_ema', 'ema', 'fixed'
    ema_alpha: 0.8  # Exponential moving average coefficient (0.8 = 80% old, 20% new)
    update_frequency: 1  # Update weights every N epochs
    # Strategy descriptions:
    # - 'equal_init': Equalize all losses at epoch 1, then keep fixed
    # - 'equal_init_ema': Equalize at epoch 1, then adapt with EMA
    # - 'ema': Always use EMA (no initial equalization)
    # - 'fixed': Use fixed weights from config (no adaptation)

# Logging
log_interval: 5  # log every N epochs
save_best: true
checkpoint_path: "best_model.pt"

# Early stopping
early_stopping:
  enabled: true
  patience: 50
  min_delta: 1.0e-6

# L-BFGS fine-tuning (runs after Adam completes)
use_lbfgs_after_adam: true  # Enable L-BFGS fine-tuning after Adam
lbfgs_lr: 0.1  # Learning rate for L-BFGS
lbfgs_max_iter: 10  # Max iterations per L-BFGS step (reduced for speed)
lbfgs_max_eval: 15  # Max function evaluations per step
lbfgs_tolerance_grad: 1.0e-7  # Gradient tolerance
lbfgs_tolerance_change: 1.0e-9  # Parameter change tolerance
lbfgs_history_size: 50  # History size for quasi-Newton approximation (reduced for memory)
lbfgs_epochs: 20  # Number of L-BFGS epochs (reduced from 50)
lbfgs_batches_per_step: 5  # Number of batches to use per L-BFGS step (subset for speed)
