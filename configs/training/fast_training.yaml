# Training configuration
epochs: 3
learning_rate: 0.01
weight_decay: 1.0e-5
optimizer: "adam"  # "adam", "sgd", etc.

# Numerical stability
grad_clip_norm: 1.0  # Gradient clipping max norm (prevents exploding gradients)
scale_lr_with_dataset: true  # Automatically scale LR based on dataset size
lr_baseline_size: 1000  # Baseline dataset size for LR scaling (adjust to your typical num_graphs)

# Loss weights
loss:
  use_pi: false
  w1_PI: 1.323e+02  # weight for displacement loss
  w2_PI: 6.981e+02  # weight for velocity loss
  use_energy: true  # Enable energy conservation loss
  w_energy: 1.0e+03  # weight for energy loss
  use_rk4: false
  w1_rk4: 1.323e+02
  w2_rk4: 6.981e+02
  use_gn_solver: false
  
  # Adaptive loss weighting
  adaptive:
    enabled: true  # Enable adaptive weighting
    strategy: 'equal_init'  # Options: 'equal_init', 'equal_init_ema', 'ema', 'fixed'
    ema_alpha: 0.8  # Exponential moving average coefficient (0.8 = 80% old, 20% new)
    update_frequency: 1  # Update weights every N epochs
    # Strategy descriptions:
    # - 'equal_init': Equalize all losses at epoch 1, then keep fixed
    # - 'equal_init_ema': Equalize at epoch 1, then adapt with EMA
    # - 'ema': Always use EMA (no initial equalization)
    # - 'fixed': Use fixed weights from config (no adaptation)

# Logging
log_interval: 5  # log every N epochs
save_best: true
checkpoint_path: "best_model.pt"

# Early stopping
early_stopping:
  enabled: true
  patience: 50
  min_delta: 1.0e-6
